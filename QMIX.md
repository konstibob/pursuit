# QMIX: Multi-Agent Reinforcement Learning

This document explains the implementation of the QMIX algorithm in this codebase and answers your questions about how it learns, why there is only one model, and how the neural networks are structured.

## 1. Why only one model? (Parameter Sharing)

You noticed that there is only one model saved in `trained_agents`. This is a common and powerful technique in Multi-Agent RL called **Parameter Sharing**.

### How it works:
Instead of training $N$ separate neural networks for $N$ agents, we use **one single network** (`AgentNetwork` in `networks.py`) and let all agents "use" it.

- **Efficiency**: It drastically reduces the number of parameters to learn.
- **Identical Agents with Unique Identities**: While the agents share the same "brain" structure, we now pass a **One-hot Agent ID** as an additional input. This allows the network to learn different roles for different agents (e.g., Agent 1 might learn to be a pusher while Agent 2 learns to block).
- **Concatenated Input**: The network input is now `Observation (147) + Agent ID (N_Agents)`.
- **How they act separately**: Each agent gets its own **Local Observation** plus its **Unique ID**. Even though they use the same weights, the different inputs (different view + different ID) lead to unique actions.

## 2. Centralized Training, Decentralized Execution (CTDE)

QMIX follows the CTDE paradigm:
- **During Training (Centralized)**: We process all agents together in a batch for computational efficiency. However, each agent still only "sees" its own observation and ID. The **Mixing Network** (Mixer) uses the **Global State** to coordinate the team's learning, but the Mixer is only used to calculate the loss.
- **During Evaluation (Decentralized)**: When you run the model in evaluation mode, the Mixer is completely ignored. Each agent independently passes its local observation and its own ID through the `AgentNetwork` to choose an action. They do **not** look at the global state or other agents' observations.

## 3. How the Learning Works (The Mixer)

The core "trick" of QMIX is how it combines individual $Q$-values into a single global $Q_{tot}$.

### The Neural Network Architecture (`networks.py`)

1. **`AgentNetwork`**: Each agent $i$ takes its observation $o_i$ and outputs $Q_i(o_i, a_i)$ for each possible action.
2. **`QMatrixMixer`**: This "mixes" the individual $Q$-values from all $N$ agents into one $Q_{tot}$.
   - It takes the $Q$-values $(Q_1, Q_2, ..., Q_N)$ and the **Global State** as inputs.
   - It outputs a single scalar $Q_{tot}$.

### The Monotonicity Constraint
For the agents to be able to make optimal decisions individually, QMIX requires that:
$$\frac{\partial Q_{tot}}{\partial Q_i} \ge 0$$
This means that an increase in any agent's individual $Q$-value must also increase the total team $Q_{tot}$.

**In the code (`networks.py`):**
```python
# Layer 1 weights
w1 = torch.abs(self.hyper_w1(states)) # Uses torch.abs() to ensure weights are positive!
```
The weights of the mixing network are generated by **Hypernetworks** (which take the Global State as input) and are then passed through `torch.abs()`. This ensures the monotonicity constraint is always satisfied.

## 4. The Loss Function (`agent.py`)

The learning happens by minimizing the **TD Error** (Temporal Difference) on the $Q_{tot}$:

1. **Collect Experience**: Store $(\text{obs, state, actions, reward, next\_obs, next\_state, done})$ in a buffer.
2. **Calculate Current $Q_{tot}$**:
   - Get $Q_i$ for each agent using `AgentNetwork`.
   - Mix them using `Mixer` + `State` $\rightarrow Q_{tot}$.
3. **Calculate Target $Q_{tot}$**:
   - Use a **Target Network** (a delayed copy of the main network) to calculate the $Q$-values for the next state.
   - $Y = \text{Reward} + \gamma \cdot Q_{tot, \text{target}}(\text{next\_state})$.
4. **Optimize**:
   - `loss = MSE(Q_{tot}, Y)`
   - Both the `AgentNetwork` and the `Mixer` are updated together using this single loss.

## Is this "Real" QMIX?

Yes, this is a very standard implementation of **QMIX**. It's not a variant like VDN (which just adds the Q-values) or Weighted QMIX. It uses the classic hypernetwork-based mixing architecture described in the original 2018 paper by Rashid et al.
